{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hello and welcome to another file we're working on together! üòÄ\n",
        "\n",
        "In this one, I'm sharing some handy (and sometimes life-saving) functions for data cleaning in NLP tasks.\n",
        "Ready to clean up that messy text? Let's dive in!"
      ],
      "metadata": {
        "id": "I8ASCNv7Hblt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext-langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28b6i-9bWaAn",
        "outputId": "8fd75edc-cbb0-4358-a089-8f3c291704ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext-langdetect\n",
            "  Downloading fasttext-langdetect-1.0.5.tar.gz (6.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fasttext>=0.9.1 (from fasttext-langdetect)\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-langdetect) (2.32.3)\n",
            "Collecting pybind11>=2.2 (from fasttext>=0.9.1->fasttext-langdetect)\n",
            "  Using cached pybind11-2.13.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext>=0.9.1->fasttext-langdetect) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext>=0.9.1->fasttext-langdetect) (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->fasttext-langdetect) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->fasttext-langdetect) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->fasttext-langdetect) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->fasttext-langdetect) (2024.8.30)\n",
            "Using cached pybind11-2.13.5-py3-none-any.whl (240 kB)\n",
            "Building wheels for collected packages: fasttext-langdetect, fasttext\n",
            "  Building wheel for fasttext-langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext-langdetect: filename=fasttext_langdetect-1.0.5-py3-none-any.whl size=7504 sha256=3b9da1b8cce7e1aa3982b7856f609cbed36b7f44aae2baba30f9878731d66085\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/5b/5d/47e2fd5c2ff1028722739ce35f365e8f6eeb89ec97aa63e621\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4246560 sha256=5cd7bdd88d67ca3ef42b3a91b4361b98129d5ddfae8d8aa8874fc88bfb0f7d2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext-langdetect fasttext\n",
            "Installing collected packages: pybind11, fasttext, fasttext-langdetect\n",
            "Successfully installed fasttext-0.9.3 fasttext-langdetect-1.0.5 pybind11-2.13.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCleaner:\n",
        "    \"\"\"\n",
        "    A collection of text cleaning functions for NLP tasks, including removing non-UTF8 characters,\n",
        "    dates, times, URLs, and more. This class also initializes a language prediction model for\n",
        "    checking if text is in English.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model=None):\n",
        "        \"\"\"\n",
        "        Initializes the TextCleaner class, importing necessary modules and setting up\n",
        "        a language model for identifying English text.\n",
        "\n",
        "        Args:\n",
        "            model: A language prediction model for detecting if text is in English.\n",
        "                   If no model is provided, the English detection functionality will be unavailable.\n",
        "        \"\"\"\n",
        "        # Import necessary modules\n",
        "        global re\n",
        "        import re  # Importing regex module for text cleaning\n",
        "\n",
        "        #self.model = model  # Setting up the language model for English identification\n",
        "\n",
        "    def is_english(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if the given text is in English using a language prediction model.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to be analyzed.\n",
        "            model: A language prediction model that returns a language code for the text.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the text is in English, False otherwise.\n",
        "        \"\"\"\n",
        "        # Predicting the language code using the model and joining the result into a single string\n",
        "        from ftlangdetect import detect\n",
        "\n",
        "        result = detect(text, low_memory=False)\n",
        "\n",
        "        # Checking if the predicted language is English ('en')\n",
        "        return result['lang'] == 'en'\n",
        "\n",
        "    def remove_hashtags(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Removes hashtags and mentions (starting with # or @) from the given text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which hashtags and mentions should be removed.\n",
        "\n",
        "        Returns:\n",
        "            str: The cleaned text with hashtags and mentions removed.\n",
        "        \"\"\"\n",
        "        # Using regex to remove any word that starts with # or @, including any leading spaces\n",
        "        cleaned_text = re.sub(r\"(?:^|\\s)[ÔºÉ#@]{1}(\\w+)\", '', text)\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "    def remove_non_utf8(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Removes non-UTF8 and non-ASCII characters from the input text.\n",
        "        Also removes apostrophes and any unwanted symbols, leaving only ASCII characters.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which non-UTF8 and non-ASCII characters should be removed.\n",
        "\n",
        "        Returns:\n",
        "            str: The cleaned text with only valid ASCII characters.\n",
        "        \"\"\"\n",
        "        # Remove a specific UTF-8 encoded apostrophe pattern\n",
        "        text = re.sub(r'\\xe2\\x80\\x99', '', text)\n",
        "\n",
        "        # Remove non-ASCII characters (those outside the range of \\x00 to \\x7f)\n",
        "        text = re.sub(r'[^\\x00-\\x7f]+', '', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_anything_between(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Removes any text between parentheses and also removes HTML tags from the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which content between parentheses and HTML tags should be removed.\n",
        "\n",
        "        Returns:\n",
        "            str: The cleaned text without content between parentheses and without HTML tags.\n",
        "        \"\"\"\n",
        "        # Remove anything inside parentheses\n",
        "        text = re.sub(r'\\((.*?)\\)', '', text)\n",
        "\n",
        "        # Remove HTML tags by finding anything between angle brackets and replacing it with a space\n",
        "        text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_date(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Replaces different date formats in the text with the word 'date'.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which dates should be removed.\n",
        "\n",
        "        Returns:\n",
        "            str: The text with dates replaced by the word 'date'.\n",
        "        \"\"\"\n",
        "        # Replace ISO format dates like \"2007-05-20\" with 'date'\n",
        "        text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', 'date', text)\n",
        "\n",
        "        # Replace dates in the format 20-05-2007 or 05-20-07 with 'date'\n",
        "        text = re.sub(r'\\d{2}-\\d{2}-\\d{2,4}', 'date', text)\n",
        "\n",
        "        # Replace dates in the format 20/05/2007 with 'date'\n",
        "        text = re.sub(r'\\d{2}/\\d{2}/\\d{2,4}', 'date', text)\n",
        "\n",
        "        # Replace dates in the format 20-May-2007 with 'date'\n",
        "        text = re.sub(r'\\d{2}-[A-Za-z]{3,}-\\d{2,4}', 'date', text)\n",
        "\n",
        "        # Replace dates with ordinal numbers like \"20th May 2007\", \"1st\", \"2nd\", \"3rd\" with 'date'\n",
        "        text = re.sub(r'\\d{1,2}(st|nd|rd|th)? [A-Za-z]+ \\d{4}', 'date', text)\n",
        "\n",
        "        # Replace dates in the format \"Sunday, May 20, 2007\" or \"20 May 2007\" with 'date'\n",
        "        text = re.sub(r'([A-Za-z]+,)?(\\s*[A-Za-z]+ \\d{1,2},?\\s*\\d{4})', 'date', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_hour(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Replaces time expressions in the text with the word 'hour'.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which hours (with AM/PM) should be removed.\n",
        "\n",
        "        Returns:\n",
        "            str: The text with time expressions replaced by the word 'hour'.\n",
        "        \"\"\"\n",
        "        # Replace hours like \"02:30 PM\", \"11 am\", \"11 AM\", or \"11 p.m.\" with 'hour'\n",
        "        text = re.sub(r'((([0-9]{2}:[0-9]{2})|(\\d{2}))\\s*(am|pm|AM|PM|p.m.|a.m.))', 'hour', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_url(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Removes URLs from the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which URLs should be removed.\n",
        "\n",
        "        Returns:\n",
        "            str: The text with URLs replaced by a space.\n",
        "        \"\"\"\n",
        "        # Simplified regex to match most common URL formats\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def remove_emails(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Removes email addresses from the input text.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text from which email addresses should be removed.\n",
        "\n",
        "        Returns:\n",
        "            str: The text with email addresses replaced by a space.\n",
        "        \"\"\"\n",
        "        text = re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', ' ', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "byQawO0EWs_R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Brief explanations for regex expressions:**\n",
        "\n",
        "1. From the *remove_hashtags* function:\n",
        "\n",
        "`r\"(?:^|\\s)[ÔºÉ#@]{1}(\\w+)\"`\n",
        "\n",
        "- `(?:^|\\s)` : This looks for either the start of the line (^) or any space (\\s) before the hashtag or mention. We're not capturing this part, just checking.\n",
        "- `[ÔºÉ#@]{1}` : Matches exactly one # or @ symbol (including the wide version ÔºÉ).\n",
        "- `(\\w+)` : Captures the word (letters, numbers, or underscores) right after the hashtag or mention.\n",
        "\n",
        "\n",
        "\n",
        "`r'[^\\x00-\\x7f]+'`\n",
        "\n",
        "- This strips out any characters that aren‚Äôt part of basic ASCII, like emojis üê†, accented letters (√©), or special symbols. Only standard English characters, numbers, and common symbols are kept.\n",
        "\n",
        "\n",
        "2. From the *remove_non_utf8* function:\n",
        "\n",
        "`r'\\xe2\\x80\\x99'`\n",
        "\n",
        "- This removes the fancy apostrophe (‚Äô) that often shows up in text copied from the web or word processors. It's the UTF-8 version of a regular apostrophe.\n",
        "\n",
        "\n",
        "3. From the *remove_anything_between* function:\n",
        "\n",
        "`r'\\((.*?)\\)'`\n",
        "\n",
        "Finds and removes anything inside parentheses (). The `.*?` is like saying \"grab just the stuff inside the closest pair of parentheses without being greedy.\"\n",
        "\n",
        "`r'<.*?>'`\n",
        "\n",
        "\n",
        "This one looks for HTML tags (anything between < >) and removes them. The `.*?` ensures it only grabs one tag at a time, so it doesn't go wild and remove too much!\n",
        "\n",
        "4. From the remove_date function:\n",
        "\n",
        "`r'([A-Za-z]+,)?(\\s*[A-Za-z]+ \\d{1,2},?\\s*\\d{4})'`\n",
        "\n",
        "- `([A-Za-z]+,)?` : This part optionally matches a day of the week (like \"Monday\") followed by a comma, but it‚Äôs not mandatory.\n",
        "- `\\s*[A-Za-z]+` : Matches the full month name (like \"February\") with optional leading spaces (\\s*).\n",
        "- `\\d{1,2}` : Matches the day of the month, either one or two digits (e.g., \"5\" or \"05\").\n",
        "- `,?\\s*` : Matches an optional comma and any extra spaces after the day.\n",
        "- `\\d{4}` : Matches the four-digit year (e.g., \"2008\").\n",
        "\n",
        "`r'\\d{1,2}(st|nd|rd|th)? [A-Za-z]+ \\d{4}'`\n",
        "\n",
        "\n",
        "- `\\d{1,2}` : Matches the day of the month, which can be 1 or 2 digits (e.g., 1, 23).\n",
        "- `(st|nd|rd|th)?` : Matches the ordinal suffix (st, nd, rd, th) if present (e.g., 1st, 2nd, 3rd, 4th), but it's optional (?).\n",
        "- `[A-Za-z]+` : Matches the month name (e.g., February).\n",
        "- `\\d{4}` : Matches the four-digit year (e.g., 2008)\n",
        "\n",
        "5. From the remove_hour function:\n",
        "\n",
        "`r'((([0-9]{2}:[0-9]{2})|(\\d{2}))\\s*(am|pm|AM|PM|p.m.|a.m.))'`\n",
        "\n",
        "- `([0-9]{2}:[0-9]{2})` : Matches time in the format 02:30, with two digits before and after the colon.\n",
        "- `(\\d{2})` : Matches time written with only two digits (like 11 for 11 o'clock).\n",
        "- `\\s*` : Matches any optional space between the time and the AM/PM marker.\n",
        "- `(am|pm|AM|PM|p.m.|a.m.)` : Matches AM/PM in different formats, such as lowercase (am), uppercase (PM), or even with periods (a.m., p.m.).\n",
        "\n",
        "6. From the remove_url function:\n",
        "\n",
        "`r'https?://\\S+|www\\.\\S+'`\n",
        "\n",
        "- `https?://` : Matches http:// or https:// at the beginning of the URL.\n",
        "- `\\S+` : Matches any sequence of non-whitespace characters after the http:// or https:// (the rest of the URL).\n",
        "- `|` : The OR operator, to match different patterns.\n",
        "www\\.\\S+ : Matches URLs starting with www. followed by any sequence of non-whitespace characters."
      ],
      "metadata": {
        "id": "LHlW7vi9KS5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the TextCleaner class (as provided earlier)\n",
        "cleaner = TextCleaner()\n",
        "\n",
        "# Example paragraph for testing\n",
        "text = \"Hey everyone! I just wanted to share my experience. I visited https://example.com at 02:30 PM on 1st February 2023,  and it was fantastic! #BestExperienceEver Also, I attended an event on 12-11-2018 (don't ask about that one, though).   You can contact me at john.doe@example.com or check my website www.johndoe.com. My meeting tomorrow is at 09:00 am.    See you at 3rd January 2022 or maybe on 5th March, 2021. Cheers!\"\n",
        "\n",
        "#text = \"Bug√ºn hava √ßok g√ºzel\"\n",
        "\n",
        "#text = \"Check out https://example.com at 02:30 PM, and on 1st February 2023!\"\n",
        "\n",
        "# Run all cleaning methods step by step\n",
        "cleaned_text = cleaner.remove_url(text)            # Remove URLs\n",
        "cleaned_text = cleaner.remove_hour(cleaned_text)   # Remove time expressions\n",
        "cleaned_text = cleaner.remove_date(cleaned_text)   # Remove date expressions\n",
        "cleaned_text = cleaner.remove_hashtags(cleaned_text)  # Remove hashtags\n",
        "cleaned_text = cleaner.remove_anything_between(cleaned_text)  # Remove text in parentheses and HTML tags (if any)\n",
        "cleaned_text = cleaner.remove_emails(cleaned_text)  # Remove emails\n",
        "cleaned_text = cleaner.remove_non_utf8(cleaned_text)  # Remove non-UTF8 characters\n",
        "\n",
        "# Check if the cleaned text is in English\n",
        "is_english = cleaner.is_english(cleaned_text)\n",
        "\n",
        "# Output the cleaned text and language detection result\n",
        "print(\"Cleaned Text:\\n\", cleaned_text)\n",
        "print(\"\\nIs the text in English?:\", is_english)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iyjlm5CXjyV",
        "outputId": "eb762b38-1d7e-45e5-ebb7-f58aca461dd2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            " Hey everyone! I just wanted to share my experience. I visited   at hour on date,  and it was fantastic! Also, I attended an event on date .   You can contact me at   or check my website   My meeting tomorrow is at hour.    See you at date or maybe on 5th March, 2021. Cheers!\n",
            "\n",
            "Is the text in English?: True\n"
          ]
        }
      ]
    }
  ]
}